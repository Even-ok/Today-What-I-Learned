### 20230820  

学习了PEFT在bloom这个大语言模型上面的微调代码，其实最核心要load进来的就2个：model和tokenizer

```python
model = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-7b1", 
    load_in_8bit=True, 
    device_map='auto',
)

tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-7b1")
```

注意torch环境需要装2.0以上的，才能跑peft



然后需要freeze所有参数，在使用一个重要的`LoraConfig`的库引入Lora配置，调整

```python
from peft import LoraConfig, get_peft_model 

config = LoraConfig(
    r=16, #attention heads
    lora_alpha=32, #alpha scaling
    # target_modules=["q_proj", "v_proj"], #if you know the 
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM" # set this for CLM or Seq2Seq
)

model = get_peft_model(model, config)
```







------

**如果想用更加高效的QLoRA，可以看这一篇**

[QLoRa：在消费级GPU上微调大型语言模型 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/634074939)



**如果想了解大模型优化/量化方案，可以看这一篇**

[LLM 盛行，如何优雅地训练大模型？_kaiyuan_sjtu的博客-CSDN博客](https://blog.csdn.net/Kaiyuan_sjtu/article/details/131778207)



**LoRA微调参数解析：**

[大模型高效微调-LoRA介绍 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/644524136?utm_id=0)



**LoraConfig参数介绍：**

- `r`: lora的秩，矩阵A和矩阵B相连接的宽度，r≪d；
- `lora_alpha`: 归一化超参数，lora参数 ΔWx 会以 α/r 归一化，以便减小改变 r 时需要重新训练的计算量；
- `lora_dropout`: lora层的dropout比率；
- `merge_weights`: eval模式，是否将lora矩阵的值加到原有 W0 的值上；
- `fan_in_fan_out`: 只有应用在 Conv1D 层时置为True，其他情况为False；
- `bias`: 是否可训练bias；
- `modules_to_save`: 除了lora部分外，还有哪些层可以被训练，并且需要保存；