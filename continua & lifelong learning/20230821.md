LoRA的位置：只是在feed-forward层旁边添加吗？

![img](https://miro.medium.com/v2/resize:fit:1050/0*9sRHGErm4Y5PDc2w.png)

> 此外，Transformer的权重矩阵包括attention模块里用于计算query, key, value的Wq，Wk，Wv以及多头attention的Wo,以及MLP层的权重矩阵，**LoRA只应用于attention模块中的4种权重矩阵，而且通过消融实验发现其中Wq，Wk两者不可缺失。同时，保证权重矩阵的种类的数量比起增加隐藏层维度r更为重要**，



关于LoRA应该在transformer的哪一层去用，这篇文章讲的很仔细，用更多的layer更低的秩(r)，表现的会更好

http://mingchao.wang/ShYWOOwr/



今天看了一篇公众号，讲LoRA的原理，讲的真好哇！ 低秩什么的我都觉得讲得好清楚了！还有我一直不懂的SVD

[图解大模型微调系列之：大模型低秩适配器LoRA（原理篇）]: https://mp.weixin.qq.com/s/dgD0Mr5kG0RR2mswjkaB6w



接下来看一下transformer的源码运行！