## 0614组会问题解决

*Posted on 24 Jun, 2019*

之前一直想开始写了，但是忙于毕设，没有心思开一个自己学习的repo。我也不知道什么样的格式才是比较好的，所以我先以**提问**的形式进行总结与学习吧！



**Q1: 什么是空间不变性和局部性？**

提出这个问题是因为师兄问，如果采用以来relationship的非连续patch作为attention的key，那局部性是否能保持？（貌似是这样问的，但我还是有点不能理解...）

- 空间不变性（平移不变性）： 无论哪种方法找到这个物体，都应该和物体的位置无关。卷积神经网络将这一概念系统化，不管物体在什么位置，用它的不变性来找到物体。
- 局部性：神经网络的底层只探索图像的局部区域，不在意相隔较远的关系。这些局部关系在最后阶段才进行聚合。

关于上面的那个问题，可能是在计算relationship的时候就要考虑全局了，所以就不一定会用局部性思想。但是因为那个模型在输入进来的时候用了一层卷积，所以应该还是有一点保存的。



**Q2: 图像相关性和语义相关性？ Attention里面的到底是用哪一种？**

- 图像相关性：可能颜色相近，但不属于同一个物体都会有较高的相关性
- 语义相关性：比较常用的一种，需要属于同一个类别，relation才高



**Q3: 参数高效方法**（这个因为之前写毕设，没认真听...）

> **参数高效微调方法（Parameter-Efficient Fine-Tuning，PEFT）** **可以使 PLM 高效适应各种下游应用任务，而无需微调预训练模型的所有参数**，**仅微调少量或额外的模型参数，固定大部分预训练参数，大大降低了计算和存储成本**

> PEFT 方法可以分为三类，不同的方法对 PLM 的不同部分进行下游任务的适配：
>
> - **Prefix/Prompt-Tuning**：在模型的输入或隐层添加 k 个额外可训练的前缀 tokens（这些前缀是连续的伪 tokens，不对应真实的 tokens），只训练这些前缀参数；
> - **Adapter-Tuning**：将较小的神经网络层或模块插入预训练模型的每一层，这些新插入的神经模块称为 adapter（适配器），下游任务微调时也只训练这些适配器参数；
> - **LoRA**：通过学习小参数的低秩矩阵来近似模型权重矩阵 W的参数更新，训练时只优化低秩矩阵参数。

自己总结的：

- prefix tuning : 就是对**输入数据的改进**，在输入的token前面加上几个和**任务相关的token**，来告诉模型它想要执行的是这个任务。
- adapter tuning : 有点像auto encoder那种结构，先编码到低维结构，通过非线性之后再扩张到原来的维度。（因为这个是直接连接到模型输出层尾部的，和LoRA做一下区分！）
- LoRA : 用可学习的低秩矩阵，模仿adapter那样，一个用来降维，一个用来升维，加上残差结构，保证更新的平稳性。而它的最终输出结果，是大模型固定参数的输出加上两个矩阵相乘的输出。



**Q4: 之前所说的重参数化方法是怎么做的？ 没有好好去了解..**

好好看看师兄怎么介绍的吧。。。

[丁霄汉：结构重参数化是怎么来的【深度学习】【直播回放】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1RM411B73M/?spm_id_from=333.337.search-card.all.click&vd_source=59bc58fe780345d63f332dbaf092d6c1)



**Q5: 什么叫自回归模型？**



**Q6: token merging, token pruning, offset tuning, channel pruning这些分别是什么东西？**



**Q7: FLOPs这个指标是怎么算的？**

[(60条消息) FLOPS的含义及其计算方式_flops如何计算_Wanderer001的博客-CSDN博客](https://blog.csdn.net/weixin_36670529/article/details/113064162)

```python
from fvcore.nn import FlopCountAnalysis

f1 = FlopCountAnalysis(model,input)
print(f1.total())
```

flops_counter

浮点运算数量

前向传播时所需的计算力



其它：

1. 1*1的卷积实际作用是对输入进行通道之间的线性重组，不涉及空间上的聚合运算。

[TODO]

- [ ] Q4
- [ ] Q5
- [ ] Q6